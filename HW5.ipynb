{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW5.ipynb","provenance":[],"authorship_tag":"ABX9TyN7KpnqymTPyK6Z63ECOH6+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"LAXWW4qcddQy"},"source":["**Task 1: Restricted Boltzman Machine** </br>\n","Citation: Deep Learning Book from lecture slides </br>\n","\n","```\n","@book{Goodfellow-et-al-2016,\n","    title={Deep Learning},\n","    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},\n","    publisher={MIT Press},\n","    note={\\url{http://www.deeplearningbook.org}},\n","    year={2016}\n","}\n","```\n","\n","\n","Like the general Boltzmann machine, the restricted Boltzmann machine is an energy-based model with the joint probability distribution specified by its energy function:\n","> $P\\left ( v = \\mathit{v},h = \\mathit{h} \\right ) = \\frac{1}{Z}esp\\left ( -E\\left ( v,h \\right ) \\right )$\n","\n","The energy function for an RBM is given by\n","> $E\\left ( v,h \\right ) = -b^{T}v - c^{T}h - v^TWh$\n","\n","and Z is the normalizing constant known as the partition function:\n","> $Z = \\sum_{\\overline{v}} \\sum _{\\overline{h}}esp\\left ( -E\\left ( \\overline{v},\\overline{h} \\right ) \\right )$\n","\n","It is apparent that the computing Z by exhaustively summing over all states could be computationally intractable. In the case of restricted Boltzmann machines, Long and Servedio (2010) formally proved that the partition function Z is intractable. The intractable partition function Z implies that the normalized joint probability distribution P (v) is also intractable to evaluate. Though P(v) is intractable, the bipartite graph structure of the RBM has the special property that its hidden and visible units are conditionally independent given one another. Having said this: \n","\n","> $P\\left ( h,v \\right )=\\frac{P\\left ( h,v \\right )}{P\\left ( v \\right )} $ </br>\n","> $= \\frac{1}{P\\left ( v \\right )}\\frac{1}{Z}exp\\left \\{ b^{T}v+c^{T}h+v^{T}Wh \\right \\}$ </br>\n","> $ = \\frac{1}{Z'}\\prod_{j}^{n_{h}}exp\\left \\{ c_{j}h_{j}+v^{T}W_{j}h_{j} \\right \\}$</br> \n","\n","It is now a simple matter of normalizing the distributions over the individual binary $h_{j}$.\n",">$P\\left ( h_{j}=1 | \\mathbf{v} \\right ) = \\sigma \\left ( c_{j}+\\mathbf{v}^{T}W_{j} \\right )$ </br>\n",">$P\\left ( h_{j}=1 , \\mathbf{v} \\right )=\\frac{\\widetilde{P}\\left ( h_{j}=1|\\mathbf{v} \\right )}{\\widetilde{P}\\left ( h_{j}=0|\\mathbf{v} \\right ) + \\widetilde{P}\\left ( h_{j}=1|\\mathbf{v} \\right )}$</br>\n",">$= \\frac{exp\\left \\{ c_{j}+\\mathbf{v}^{T}W_{j} \\right \\}}{exp\\left \\{ 0 \\right \\}+exp\\left \\{ c_{j}+\\mathbf{v}^{T}W_{j} \\right \\}}$</br>\n",">$=\\sigma \\left ( c_{j}+\\mathbf{v}^{T}W_{j} \\right )$</br>\n",">$=\\frac{1}{Z'}exp\\left \\{\\sum _{j=1}^{n_{h}}c_{j}h_{j} + \\sum _{j=1}^{n_{h}}v^{T}W_{j}h_{j} \\right \\}$ </br>\n","\n","A similar derivation will show that the other condition of interest to us, P(v | h): \n","> $P\\left ( v|\\mathbf{h} \\right )=\\frac{1}{Z'}\\prod _{k}exp\\left \\{ b_{k}+h^{T}W_{k} \\right \\}$\n","\n","Therefore, we can conclude:\n",">$P\\left ( v_{k}=1 | h \\right )=\\sigma \\left ( b_{k} +h^{T}W_{k} \\right )$</br>\n",">$= \\frac{1}{Z'}exp\\left \\{ c^{T}h + v^{T}Wh\\right \\}$\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LUs0OlmthWlL"},"source":["**Task 2: Variational Autoencoder** </br>\n","Let's say we want to infer P(z|X) using Q(z|X). The KL divergence is then formulated as follows:\n","> $D_{KL}\\left [ Q\\left ( z|X \\right )\\parallel P\\left ( z|X \\right ) \\right ] = \\sum _{z}Q\\left ( z|X \\right )log\\frac{Q\\left ( z|X \\right )}{P\\left ( z|X \\right )}$ </br>\n","> $=E\\left [ log\\frac{Q\\left ( z|X \\right )}{P\\left ( z|X \\right )} \\right ]$</br>\n",">$=E\\left [ logQ\\left ( z|X \\right ) - logP\\left ( z|X \\right ) \\right ]$</br>\n","\n",">$D_{KL}\\left [ Q\\left ( z|X \\right )\\parallel P\\left ( z|X \\right ) \\right ] = E\\left [ logQ\\left ( z|X \\right )-log\\frac{P\\left ( X|z \\right )P\\left ( z \\right )}{P\\left ( X \\right )} \\right ]$</br>\n",">$=E\\left [ logQ\\left ( z|X \\right )-\\left ( logP\\left ( X|z \\right ) + logP\\left ( z \\right )- logP\\left ( X \\right ) \\right ) \\right ]$</br>\n",">$=E\\left [ logQ\\left ( z|X \\right )-logP\\left ( X|z \\right ) - logP\\left ( z \\right )+ logP\\left ( X \\right ) \\right ]$</br>\n","\n","Notice that the expectation is over z and P(X) doesnâ€™t depend on z so we could move it outside of the expectation.\n",">$D_{KL}\\left [ Q\\left ( z|X \\right )\\parallel P\\left ( z|X \\right ) \\right ]=E\\left [ logQ\\left ( z|X \\right )-logP\\left ( X|z \\right )-logP\\left ( z \\right ) \\right ] +logP\\left ( X \\right )$</br>\n",">$D_{KL}\\left [ Q\\left ( z|X \\right )\\parallel P\\left ( z|X \\right ) \\right ]-logP\\left ( X \\right )=E\\left [ logQ\\left ( z|X \\right )-logP\\left ( X|z \\right )-logP\\left ( z \\right ) \\right ]$</br>\n",">$logP\\left ( X \\right )-D_{KL}\\left [ Q\\left ( z|X \\right )\\parallel P\\left ( z|X \\right ) \\right ]=E\\left [logP\\left ( X|z \\right ) -\\left ( logQ\\left ( z|X \\right )-logP\\left ( z \\right ) \\right )\\right ]$</br>\n",">$= E\\left [ logP\\left ( X|z \\right ) \\right ] - E\\left [ logQ\\left ( z|X \\right )-logP\\left ( z \\right )\\right ]$</br>\n",">$= E\\left [ logP\\left ( X|z \\right ) \\right ] - D_{KL}\\left [Q\\left ( z|X \\right )\\parallel P\\left ( z \\right )\\right ]$</br>\n","\n","And this is the VAE objective function:\n","> $logP\\left ( X \\right ) - D_{KL}\\left [ Q\\left ( z|X \\right )\\parallel P\\left ( z|X \\right ) \\right ] =E\\left [ logP\\left ( X|z \\right ) \\right ] - D_{KL}\\left [Q\\left ( z|X \\right )\\parallel P\\left ( z \\right )\\right ]$</br>\n"]}]}