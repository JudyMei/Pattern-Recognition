{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment2.ipynb","provenance":[],"authorship_tag":"ABX9TyORSMLB7FuLL+bRZfm+09yR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"kO98ClwyLzHv","executionInfo":{"status":"ok","timestamp":1615580492032,"user_tz":300,"elapsed":1305,"user":{"displayName":"Judy Mei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXND6yISPNzmvUGHcPoq9V5iN0VmjOKOIp7caSpw=s64","userId":"13598839992154869370"}}},"source":["#Setup\n","from sklearn import svm, metrics\n","from sklearn import preprocessing\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"DFikJqp3O10k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615581146627,"user_tz":300,"elapsed":653945,"user":{"displayName":"Judy Mei","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiXND6yISPNzmvUGHcPoq9V5iN0VmjOKOIp7caSpw=s64","userId":"13598839992154869370"}},"outputId":"9823d894-4433-49a4-ba78-586bed6cf70f"},"source":["#Task 1\n","\n","#Import MNIST dataset (code from Deep Learning with Python textbook)\n","from keras.datasets import mnist\n","(train_x, train_y), (test_x, test_y) = mnist.load_data()\n","train_x = train_x/255.0\n","test_x = test_x/255.0\n","train_x = train_x.reshape((train_x.shape[0],-1))\n","test_x = test_x.reshape((test_x.shape[0],-1))\n","\n","train_x = preprocessing.scale(train_x)\n","test_x = preprocessing.scale(test_x)\n","\n","var = svm.SVC(C = 0.1, kernel = 'linear', gamma='scale')\n","var.fit(train_x, train_y)\n","p = var.predict(test_x)\n","\n","acc = metrics.accuracy_score(test_y, p)\n","confusion = metrics.confusion_matrix(test_y, p)\n","print(\"Accuracy = \", acc*100, \"%\")\n","print(confusion)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","Accuracy =  93.76 %\n","[[ 958    0    5    2    1    7    5    1    1    0]\n"," [   0 1117    6    3    0    1    3    1    4    0]\n"," [   6   10  966   12    5    4    7    7   15    0]\n"," [   4    0   16  950    1   14    2    5   14    4]\n"," [   2    2   10    1  937    0    5    4    4   17]\n"," [   8    3    4   41    5  797   12    1   19    2]\n"," [  13    4   13    1    7   18  900    0    2    0]\n"," [   1    8   21   12    7    0    0  959    0   20]\n"," [   7    6    7   25    7   24    7    8  873   10]\n"," [   9    6    3    8   28    5    0   22    9  919]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4icWhHRY_o8D"},"source":["-----**Task 2**----- \n","\n","Given features: $(x_{1},y_{1}),...,(x_{N},y_{N})$ where $y_{1},...,y_{N}âˆˆ\\left \\{ -1,1 \\right \\}$ <br>\n","Minimize: $w^{T}\\cdot w+C\\sum_{i=1}^{N}\\xi _{i}$ <br> \n","> (1) *subject to:* $y_{i}\\cdot \\left ( w^{T}\\cdot x_{i} \\right )\\geq 1-\\xi_{i}$ <br>\n","> (2) *and* $\\xi_{i}\\geq 0$ for $i= 1,...,N$<br>\n","\n","We can deduce from the SVM lecture slides, the following **primal margin**:\n","> Primal Margin: $\\gamma = \\frac{1}{\\sqrt{w^{T}\\cdot w + C\\sum_{i=1}^{N}\\xi _{i}}}$\n","\n","The Lagrange function given our constraints and functions will be in the following form: <br>\n","> $L=\\frac{1}{2}w^{T}\\cdot w + C\\sum_{i=1}^{N}\\xi _{i} + \\sum _{i} \\mu (1-y_{i}w^{T}x_{i} - \\xi _{i}) - \\sum_{i=1}^{N}\\lambda _{i}\\xi _{i}$\n","<br>Lagrange Multipliers: $\\mu\\geq 0$ and $\\lambda\\geq 0$\n","\n","Claim:<br>\n","> $\\underset{\\mu ,\\lambda }{max}\\underset{w,\\xi }{min} L\\leq \\underset{w,\\xi }{min} \\underset{\\mu ,\\lambda }{max}L $\n","\n","Expanding Lagrange function:\n",">$L=\\frac{1}{2}w^{T}\\cdot w + C\\sum_{i=1}^{N}\\xi _{i}+\\sum _{i}\\mu _{i} - \\sum _{i}\\mu _{i}y_{i}w^{T}x_{i} - \\sum _{i}\\mu _{i}\\xi _{i} - \\sum_{i=1}^{N}\\lambda _{i}\\xi _{i}$<br>\n","\n","Taking the derivative of the function:\n","> $\\tfrac{\\partial L}{\\partial w} = w - \\sum _{i}\\mu _{i}y_{i}x_{i}=0\\Rightarrow w = \\sum _{i}\\mu _{i}y_{i}\\overrightarrow{x_{i}}$<br>\n","$\\tfrac{\\partial L}{\\partial \\xi } = C - \\mu _{i} - \\lambda _{i} = 0 \\Rightarrow C = \\mu _{i}+\\lambda_{i} $<br>\n","\n","Substituting $w = \\sum _{i}\\mu _{i}y_{i}\\overrightarrow{x_{i}}$ and $C = \\mu _{i}+\\lambda_{i}$ into Lagrange function, we get the dual problem of\n","maximizing:\n","> $L=\\frac{1}{2}w^{T}\\sum _{i}\\mu _{i}y_{i}\\overrightarrow{x_{i}} + \\left ( \\mu _{i}+\\lambda_{i} \\right )\\sum_{i=1}^{N}\\xi _{i}+\\sum _{i}\\mu _{i} - \\sum _{i}w^{T}\\mu _{i}y_{i}x_{i} - \\sum _{i}\\mu _{i}\\xi _{i} - \\sum_{i=1}^{N}\\lambda _{i}\\xi _{i}$<br>\n","$L=\\sum _{i}\\mu _{i}-\\frac{1}{2}w^{T}\\sum _{i}\\mu _{i}u_{i}\\overrightarrow{x_{i}}$<br>\n","$L=\\sum _{i}\\mu _{i}-\\frac{1}{2}\\sum _{i,j}\\mu _{i}\\mu _{j}y_{i}y_{j}(\\overrightarrow{x_{i}}x_{j})$\n","\n","The **dual margin** is: \n",">$\\gamma=\\frac{1}{\\sqrt{\\mu _{i}\\mu _{j}y_{i}y_{j}(\\overrightarrow{x_{i}}x_{j})}}$<br>\n","\n","**Summary**:<br>\n","> Primal Margin: $\\gamma = \\frac{1}{\\sqrt{w^{T}\\cdot w + C\\sum_{i=1}^{N}\\xi _{i}}}$<br>\n","Dual Margin: $\\gamma=\\frac{1}{\\sqrt{\\mu _{i}\\mu _{j}y_{i}y_{j}(\\overrightarrow{x_{i}}x_{j})}}$<br>\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yWYygcVB6R2u"},"source":["**Benefits of maximizing the margin:**<br>\n","* Maximizing the margin helps avoid the overfitting problem. <br>\n","\n","**Characterize the support vectors:**<br>\n","There are three possibilities:\n","1. Support vectors are lying on the wrong side of the hyperplane $\\xi _{n}\\geq 1$\n","2. Support vectors are lying within the margin boundaries $\\left ( 0< \\xi _{n}< 1 \\right )$ but still on the correct side \n","3. Support vectors are lying on the margin boundaries $\\left (\\xi _{n}=0  \\right )$<br>\n","\n","**Point out the benefit of solving the dual problem instead of the primal problem:**<br>\n","* The dual problem provides a lower bound of the primal problem \n","\n","\n"]}]}